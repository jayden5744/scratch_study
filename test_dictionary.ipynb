{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/sample/train.csv\")\n",
    "train_df[\"korean\"].to_csv(\"data/sample/train.ko\", index=False)\n",
    "train_df[\"english\"].to_csv(\"data/sample/train.en\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv(\"data/sample/valid.csv\")\n",
    "valid_df[\"korean\"].to_csv(\"data/sample/valid.ko\", index=False)\n",
    "valid_df[\"english\"].to_csv(\"data/sample/valid.en\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.datasets.data_helper import create_or_load_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_vocab = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.ko\",\n",
    "    save_path=\"dictionary/sample\",\n",
    "    language=\"ko\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "[592, 82, 2408, 4977, 1002, 7499, 1019, 10, 351, 2555, 605, 25, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "50\n",
      "['▁안녕하세요', '▁저는', '▁E', 'st', 's', 'of', 't', '의', '▁정', '환', '석', '입니다', '.']\n",
      "안녕하세요 저는 Estsoft의 정환석입니다.\n"
     ]
    }
   ],
   "source": [
    "print(ko_vocab.GetPieceSize())\n",
    "text = \"안녕하세요 저는 Estsoft의 정환석입니다.\"\n",
    "idx_lst = ko_vocab.EncodeAsIds(text)\n",
    "print(idx_lst + [4] * (50 - len(idx_lst)))\n",
    "print(len(idx_lst + [4] * (50 - len(idx_lst))))\n",
    "print(ko_vocab.EncodeAsPieces(text))\n",
    "print(ko_vocab.DecodeIds(idx_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.en\",\n",
    "    save_path=\"dictionary/sample\",\n",
    "    language=\"en\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "[952, 69, 408, 17, 23, 2]\n",
      "['▁Hello', '▁my', '▁name', '▁is', '▁', '정환석']\n",
      "Hello my name is  ⁇ \n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.GetPieceSize())\n",
    "text = \"Hello my name is 정환석\"\n",
    "idx_lst = en_vocab.EncodeAsIds(text)\n",
    "print(idx_lst)\n",
    "print(en_vocab.EncodeAsPieces(text))\n",
    "print(en_vocab.DecodeIds(idx_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_vocab_bpe = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.ko\",\n",
    "    save_path=\"dictionary/sample_bpe\",\n",
    "    language=\"ko\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "[844, 207, 1781, 5080, 7230, 7107, 7490, 7158, 6736, 48, 6948, 7014, 18, 6717]\n",
      "['▁안녕하세요', '▁저는', '▁E', 'st', 's', 'o', 'f', 't', '의', '▁정', '환', '석', '입니다', '.']\n",
      "안녕하세요 저는 Estsoft의 정환석입니다.\n"
     ]
    }
   ],
   "source": [
    "print(ko_vocab_bpe.GetPieceSize())\n",
    "text = \"안녕하세요 저는 Estsoft의 정환석입니다.\"\n",
    "idx_lst = ko_vocab_bpe.EncodeAsIds(text)\n",
    "print(idx_lst)\n",
    "print(ko_vocab_bpe.EncodeAsPieces(text))\n",
    "print(ko_vocab_bpe.DecodeIds(idx_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_vocab_char = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.ko\",\n",
    "    save_path=\"dictionary/sample_char\",\n",
    "    language=\"ko\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"char\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "[4, 76, 289, 10, 73, 17, 4, 41, 11, 4, 507, 518, 446, 518, 395, 778, 446, 24, 4, 56, 236, 302, 50, 7, 6, 5]\n",
      "['▁', '안', '녕', '하', '세', '요', '▁', '저', '는', '▁', 'E', 's', 't', 's', 'o', 'f', 't', '의', '▁', '정', '환', '석', '입', '니', '다', '.']\n",
      "안녕하세요 저는 Estsoft의 정환석입니다.\n"
     ]
    }
   ],
   "source": [
    "print(ko_vocab_char.GetPieceSize())\n",
    "text = \"안녕하세요 저는 Estsoft의 정환석입니다.\"\n",
    "idx_lst = ko_vocab_char.EncodeAsIds(text)\n",
    "print(idx_lst)\n",
    "print(ko_vocab_char.EncodeAsPieces(text))\n",
    "print(ko_vocab_char.DecodeIds(idx_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_vocab_word = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.ko\",\n",
    "    save_path=\"dictionary/sample_word\",\n",
    "    language=\"ko\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "[720, 23, 2]\n",
      "['▁안녕하세요', '▁저는', '▁Estsoft의▁정환석입니다.']\n",
      "안녕하세요 저는 ⁇ \n"
     ]
    }
   ],
   "source": [
    "print(ko_vocab_word.GetPieceSize())\n",
    "text = \"안녕하세요 저는 Estsoft의 정환석입니다.\"\n",
    "idx_lst = ko_vocab_word.EncodeAsIds(text)\n",
    "print(idx_lst)\n",
    "print(ko_vocab_word.EncodeAsPieces(text))\n",
    "print(ko_vocab_word.DecodeIds(idx_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.datasets.data_helper import create_or_load_tokenizer\n",
    "\n",
    "ko_vocab = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.ko\",\n",
    "    save_path=\"dictionary/sample\",\n",
    "    language=\"ko\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"unigram\"\n",
    ")\n",
    "\n",
    "en_vocab = create_or_load_tokenizer(\n",
    "    file_path=\"data/sample/train.en\",\n",
    "    save_path=\"dictionary/sample\",\n",
    "    language=\"en\",\n",
    "    vocab_size=8000,\n",
    "    tokenizer_type=\"unigram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.datasets.data_helper import TrainDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrainDataset(\n",
    "        x_path=\"data/sample/train.ko\",\n",
    "        src_vocab=ko_vocab,\n",
    "        y_path=\"data/sample/train.en\",\n",
    "        trg_vocab=en_vocab,\n",
    "        max_sequence_size=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   8,  446, 1946,   15,  169, 3318, 1110, 1946,  529,    4,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      ">저 머리가 진짜 멋있는 머리인데.\n",
      "tensor([[   0,   19,  177,  473,  156,   30,   12,  120,  705,  473, 3443,    4,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      ">That hair should be a really cool hairstyle.\n",
      "tensor([[  19,  177,  473,  156,   30,   12,  120,  705,  473, 3443,    4,    1,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      ">That hair should be a really cool hairstyle.\n",
      "tensor([[ 379,   12,    5, 7853,  390, 5237,   34, 2572,    4,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      "그들이 컸던 과거와 달리.\n",
      "tensor([[   0,   23, 2885,   20,    6,  855,  277,  148,  143,  376,    4,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      "Unlike in the past where they were large.\n",
      "tensor([[  23, 2885,   20,    6,  855,  277,  148,  143,  376,    4,    1,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])\n",
      "Unlike in the past where they were large.\n"
     ]
    }
   ],
   "source": [
    "sampler = RandomSampler(dataset)\n",
    "loader = DataLoader(dataset=dataset, batch_size=1, sampler=sampler)\n",
    "idx =0\n",
    "for i in loader:\n",
    "    encoder_input, decoder_input, decoder_output = i\n",
    "    print(encoder_input)\n",
    "    print(ko_vocab.DecodeIds(encoder_input[0].tolist()))\n",
    "    print(decoder_input)\n",
    "    print(en_vocab.DecodeIds(decoder_input[0].tolist()))\n",
    "    print(decoder_output)\n",
    "    print(en_vocab.DecodeIds(decoder_output[0].tolist()))\n",
    "    idx += 1\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5505d05ceda49dc504182582297c3c948daaaa09c5b6c6e672bebc3df7629cc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
